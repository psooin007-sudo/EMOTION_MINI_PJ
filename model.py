"""
ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ëª¨ë“ˆ - í†µí•© ìµœì í™” ë²„ì „
web_with_graphs.pyì™€ í˜¸í™˜ë˜ëŠ” í†µí•© ëª¨ë¸ ë§¤ë‹ˆì €
"""

import cv2
import numpy as np
from PIL import Image
import streamlit as st
import threading
import time
import logging
from typing import Tuple, Optional, Union
from functools import lru_cache
import warnings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FaceTracker:
    """ì•ˆì •ì ì¸ ë‹¨ì¼ ì–¼êµ´ ì¶”ì ì„ ìœ„í•œ í´ë˜ìŠ¤ - web_with_graphs.pyì™€ ë™ì¼"""
    
    def __init__(self, max_missing_frames=10, min_overlap_ratio=0.3):
        self.tracked_face = None
        self.missing_frames = 0
        self.max_missing_frames = max_missing_frames
        self.min_overlap_ratio = min_overlap_ratio
        self.face_history = []
        self.max_history = 5
        
    def calculate_overlap_ratio(self, rect1, rect2):
        """ë‘ ì‚¬ê°í˜•ì˜ ê²¹ì¹˜ëŠ” ë¹„ìœ¨ ê³„ì‚°"""
        x1_1, y1_1, w1, h1 = rect1
        x2_1, y2_1 = x1_1 + w1, y1_1 + h1
        
        x1_2, y1_2, w2, h2 = rect2
        x2_2, y2_2 = x1_2 + w2, y1_2 + h2
        
        # ê²¹ì¹˜ëŠ” ì˜ì—­ ê³„ì‚°
        overlap_x1 = max(x1_1, x1_2)
        overlap_y1 = max(y1_1, y1_2)
        overlap_x2 = min(x2_1, x2_2)
        overlap_y2 = min(y2_1, y2_2)
        
        if overlap_x1 >= overlap_x2 or overlap_y1 >= overlap_y2:
            return 0.0
        
        overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
        rect1_area = w1 * h1
        
        return overlap_area / rect1_area if rect1_area > 0 else 0.0
    
    def find_best_face(self, faces):
        """ê°€ì¥ ì í•©í•œ ì–¼êµ´ ì„ íƒ"""
        if len(faces) == 0:
            return None
        
        if self.tracked_face is None:
            return max(faces, key=lambda x: x[2] * x[3])
        
        # ê¸°ì¡´ ì¶”ì  ì–¼êµ´ê³¼ ê²¹ì¹˜ëŠ” ì–¼êµ´ë“¤ ì°¾ê¸°
        matching_faces = []
        for face in faces:
            overlap = self.calculate_overlap_ratio(self.tracked_face, face)
            if overlap >= self.min_overlap_ratio:
                matching_faces.append((face, overlap))
        
        if matching_faces:
            return max(matching_faces, key=lambda x: x[1])[0]
        else:
            return max(faces, key=lambda x: x[2] * x[3])
    
    def update_tracking(self, faces, scale=1.0):
        """ì–¼êµ´ ì¶”ì  ì—…ë°ì´íŠ¸"""
        if len(faces) == 0:
            self.missing_frames += 1
            if self.missing_frames > self.max_missing_frames:
                self.tracked_face = None
                self.face_history.clear()
            return self.tracked_face
        
        # ìŠ¤ì¼€ì¼ ì¡°ì •
        scaled_faces = []
        for x, y, w, h in faces:
            if scale != 1.0:
                x, y, w, h = int(x/scale), int(y/scale), int(w/scale), int(h/scale)
            scaled_faces.append((x, y, w, h))
        
        best_face = self.find_best_face(scaled_faces)
        
        if best_face is not None:
            self.tracked_face = best_face
            self.missing_frames = 0
            
            self.face_history.append(best_face)
            if len(self.face_history) > self.max_history:
                self.face_history.pop(0)
        
        return self.tracked_face
    
    def get_stable_face(self):
        """ì•ˆì •í™”ëœ ì–¼êµ´ ìœ„ì¹˜ ë°˜í™˜"""
        if not self.face_history:
            return self.tracked_face
        
        # ìµœê·¼ ì–¼êµ´ ìœ„ì¹˜ë“¤ì˜ í‰ê· ìœ¼ë¡œ ì•ˆì •í™”
        avg_x = sum(face[0] for face in self.face_history) // len(self.face_history)
        avg_y = sum(face[1] for face in self.face_history) // len(self.face_history)
        avg_w = sum(face[2] for face in self.face_history) // len(self.face_history)
        avg_h = sum(face[3] for face in self.face_history) // len(self.face_history)
        
        return (avg_x, avg_y, avg_w, avg_h)
    
    def reset(self):
        """íŠ¸ë˜ì»¤ ë¦¬ì…‹"""
        self.tracked_face = None
        self.missing_frames = 0
        self.face_history.clear()


class UnifiedModelManager:
    """í†µí•© ëª¨ë¸ ê´€ë¦¬ ì‹±ê¸€í†¤ í´ë˜ìŠ¤ - ì¤‘ë³µ ë¡œë”© ë°©ì§€"""
    _instance = None
    _lock = threading.RLock()  # ì¬ì§„ì… ê°€ëŠ¥í•œ ë½
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized') and self._initialized:
            return
        
        self._emotion_pipeline = None
        self._face_cascade = None
        self._model_id = "dima806/facial_emotions_image_detection"
        self._device = -1  # CPU ì‚¬ìš©
        self._initialized = True
        
        logger.info("ğŸ—ï¸ UnifiedModelManager ì´ˆê¸°í™”")
    
    @property
    def emotion_pipeline(self):
        """ì§€ì—° ë¡œë”©ìœ¼ë¡œ ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë°˜í™˜"""
        if self._emotion_pipeline is None:
            self._emotion_pipeline = self._load_emotion_model()
        return self._emotion_pipeline
    
    @property 
    def face_cascade(self):
        """ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì–¼êµ´ ê°ì§€ê¸° ë°˜í™˜"""
        if self._face_cascade is None:
            self._face_cascade = self._load_face_cascade()
        return self._face_cascade
    
    def _load_emotion_model(self):
        """ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info("ğŸ”„ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # ê²½ê³  ë©”ì‹œì§€ ì–µì œ
            warnings.filterwarnings("ignore", category=UserWarning)
            
            from transformers import pipeline, AutoImageProcessor
            
            # Streamlit í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì‚¬ìš©
            if 'st' in globals() and hasattr(st, 'cache_resource'):
                @st.cache_resource
                def create_pipeline():
                    processor = AutoImageProcessor.from_pretrained(self._model_id, use_fast=True)
                    return pipeline(
                        'image-classification', 
                        model=self._model_id,
                        image_processor=processor,
                        device=self._device,
                        return_all_scores=False  # ì„±ëŠ¥ í–¥ìƒ
                    )
                pipe = create_pipeline()
            else:
                # ì¼ë°˜ í™˜ê²½ì—ì„œëŠ” ì§ì ‘ ìƒì„±
                processor = AutoImageProcessor.from_pretrained(self._model_id, use_fast=True)
                pipe = pipeline(
                    'image-classification', 
                    model=self._model_id,
                    image_processor=processor,
                    device=self._device,
                    return_all_scores=False
                )
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            test_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
            test_result = pipe(test_img)
            
            logger.info(f"âœ… ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! í…ŒìŠ¤íŠ¸: {test_result[0]['label'] if test_result else 'No result'}")
            return pipe
            
        except ImportError as e:
            error_msg = "transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            logger.error(f"{error_msg}: {e}")
            if 'st' in globals():
                st.error(error_msg)
            return None
        except Exception as e:
            error_msg = f"ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            if 'st' in globals():
                st.error(error_msg)
            return None
    
    def _load_face_cascade(self):
        """ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ"""
        try:
            logger.info("ğŸ”„ ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì¤‘...")
            
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            face_cascade = cv2.CascadeClassifier(cascade_path)
            
            if face_cascade.empty():
                raise ValueError("ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì‹¤íŒ¨")
            
            logger.info("âœ… ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì™„ë£Œ!")
            return face_cascade
            
        except Exception as e:
            logger.error(f"ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    def get_emotion_pipeline(self):
        """ì™¸ë¶€ í˜¸ì¶œìš© - web_with_graphs.py í˜¸í™˜ì„±"""
        return self.emotion_pipeline
    
    def get_face_cascade(self):
        """ì™¸ë¶€ í˜¸ì¶œìš© - web_with_graphs.py í˜¸í™˜ì„±"""
        return self.face_cascade


class EmotionAnalyzer:
    """í†µí•© ê°ì • ë¶„ì„ í´ë˜ìŠ¤"""
    
    # ê°ì • ë§¤í•‘ (ëª¨ë¸ ì¶œë ¥ -> ì•± í˜•ì‹)
    EMOTION_MAPPING = {
        'angry': 'angry',
        'anger': 'angry',
        'sad': 'sad',
        'sadness': 'sad',
        'happy': 'happy',
        'happiness': 'happy',
        'fear': 'fear',
        'fearful': 'fear',
        'surprise': 'surprise',
        'surprised': 'surprise',
        'neutral': 'neutral',
        'disgust': 'disgust',
        'disgusted': 'disgust'
    }
    
    def __init__(self):
        self._model_manager = UnifiedModelManager()
        self._latest_emotion = None
        self._latest_confidence = 0.0
        self._lock = threading.RLock()
        self._face_tracker = FaceTracker()
        
        logger.info("ğŸ­ EmotionAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
    
    @property
    def pipeline(self):
        """ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë°˜í™˜"""
        return self._model_manager.emotion_pipeline
    
    @property 
    def face_cascade(self):
        """ì–¼êµ´ ê°ì§€ê¸° ë°˜í™˜"""
        return self._model_manager.face_cascade
    
    def _preprocess_image(self, image: Image.Image, target_size: Tuple[int, int] = (224, 224)) -> Image.Image:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìµœì í™”"""
        try:
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
            if image.size != target_size:
                image = image.resize(target_size, Image.Resampling.LANCZOS)
            
            # RGB ë³€í™˜ í™•ì¸
            if image.mode != 'RGB':
                image = image.convert('RGB')
                
            return image
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return image
    
    def analyze_emotion(self, image: Union[Image.Image, np.ndarray]) -> Tuple[str, float]:
        """
        ì´ë¯¸ì§€ì—ì„œ ê°ì • ë¶„ì„
        Args:
            image: PIL Image ë˜ëŠ” numpy array
        Returns:
            tuple: (emotion_key, confidence)
        """
        if self.pipeline is None:
            return 'neutral', 0.0
        
        try:
            # numpy arrayì¸ ê²½ìš° PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                if len(image.shape) == 3 and image.shape[2] == 3:
                    # BGR -> RGB ë³€í™˜
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image = Image.fromarray(image)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_image = self._preprocess_image(image)
            
            # ê°ì • ë¶„ì„ ì‹¤í–‰
            results = self.pipeline(processed_image)
            
            if results and len(results) > 0:
                top_result = results[0]
                emotion = top_result['label'].lower()
                confidence = float(top_result['score'])
                
                # ê°ì • ë§¤í•‘
                mapped_emotion = self.EMOTION_MAPPING.get(emotion, 'neutral')
                
                logger.debug(f"ê°ì • ë¶„ì„: {emotion} -> {mapped_emotion} ({confidence:.2f})")
                return mapped_emotion, confidence
            
            return 'neutral', 0.0
            
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 'neutral', 0.0
    
    def detect_face_and_analyze(self, image_array: np.ndarray, 
                               min_face_size: Tuple[int, int] = (50, 50),
                               use_tracking: bool = True) -> Tuple[str, float, Optional[Tuple[int, int, int, int]]]:
        """
        ë‹¨ì¼ ì–¼êµ´ ê°ì§€ ë° ê°ì • ë¶„ì„
        Args:
            image_array: OpenCV BGR ì´ë¯¸ì§€
            min_face_size: ìµœì†Œ ì–¼êµ´ í¬ê¸°
            use_tracking: ì–¼êµ´ ì¶”ì  ì‚¬ìš© ì—¬ë¶€
        Returns:
            tuple: (emotion_key, confidence, face_coordinates)
        """
        if self.face_cascade is None:
            logger.warning("ì–¼êµ´ ê°ì§€ê¸°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 'neutral', 0.0, None
        
        try:
            # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
            height, width = image_array.shape[:2]
            max_width = 640
            
            if width > max_width:
                scale = max_width / width
                new_width = int(width * scale)
                new_height = int(height * scale)
                resized_img = cv2.resize(image_array, (new_width, new_height))
            else:
                scale = 1.0
                resized_img = image_array
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)
            
            # ì–¼êµ´ ê°ì§€
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.2, 
                minNeighbors=5,
                minSize=min_face_size,
                maxSize=(min(width//2, 400), min(height//2, 400)),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            # ì–¼êµ´ ì¶”ì  ë˜ëŠ” ì„ íƒ
            if use_tracking:
                tracked_face = self._face_tracker.update_tracking(faces, scale)
                target_face = tracked_face
            else:
                # ì¶”ì  ì—†ì´ ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
                target_face = max(faces, key=lambda x: x[2] * x[3]) if len(faces) > 0 else None
            
            if target_face is not None:
                x, y, w, h = target_face
                
                # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ (ì—¬ë°± ì¶”ê°€)
                margin = max(w, h) // 10
                x1, y1 = max(0, x - margin), max(0, y - margin)
                x2, y2 = min(image_array.shape[1], x + w + margin), min(image_array.shape[0], y + h + margin)
                
                face_img = image_array[y1:y2, x1:x2]
                
                # ì–¼êµ´ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìŠ¤í‚µ
                if face_img.shape[0] < 50 or face_img.shape[1] < 50:
                    return 'neutral', 0.0, None
                
                # ê°ì • ë¶„ì„
                emotion, confidence = self.analyze_emotion(face_img)
                
                return emotion, confidence, target_face
            
            return 'neutral', 0.0, None
            
        except Exception as e:
            logger.error(f"ì–¼êµ´ ê°ì§€ ë° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 'neutral', 0.0, None
    
    def update_emotion_state(self, emotion: str, confidence: float) -> None:
        """ìµœì‹  ê°ì • ìƒíƒœ ì—…ë°ì´íŠ¸ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._lock:
            self._latest_emotion = emotion
            self._latest_confidence = float(confidence)
    
    def get_emotion_state(self) -> Tuple[Optional[str], float]:
        """ìµœì‹  ê°ì • ìƒíƒœ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._lock:
            return self._latest_emotion, self._latest_confidence
    
    def reset_emotion_state(self) -> None:
        """ê°ì • ìƒíƒœ ì´ˆê¸°í™”"""
        with self._lock:
            self._latest_emotion = None
            self._latest_confidence = 0.0
            self._face_tracker.reset()
    
    def reset_face_tracking(self) -> None:
        """ì–¼êµ´ ì¶”ì ë§Œ ë¦¬ì…‹"""
        self._face_tracker.reset()


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_emotion_analyzer = None
_model_manager = None

def get_emotion_analyzer() -> EmotionAnalyzer:
    """EmotionAnalyzer ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _emotion_analyzer
    if _emotion_analyzer is None:
        _emotion_analyzer = EmotionAnalyzer()
    return _emotion_analyzer

def get_model_manager() -> UnifiedModelManager:
    """ModelManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _model_manager
    if _model_manager is None:
        _model_manager = UnifiedModelManager()
    return _model_manager


# === í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ë“¤ ===

@lru_cache(maxsize=1)
def load_emotion_model():
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ - Streamlit ìºì‹œ ì§€ì›"""
    if 'st' in globals():
        # Streamlit í™˜ê²½
        return get_model_manager().emotion_pipeline
    else:
        # ì¼ë°˜ í™˜ê²½
        return get_emotion_analyzer().pipeline

def analyze_emotion_from_image(image: Image.Image) -> Tuple[str, float]:
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    analyzer = get_emotion_analyzer()
    return analyzer.analyze_emotion(image)

def detect_face_and_analyze(image_array: np.ndarray) -> Tuple[str, float, Optional[Tuple[int, int, int, int]]]:
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    analyzer = get_emotion_analyzer()
    return analyzer.detect_face_and_analyze(image_array)

def update_latest_emotion(emotion: str, confidence: float) -> None:
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    analyzer = get_emotion_analyzer()
    analyzer.update_emotion_state(emotion, confidence)

def get_latest_emotion() -> Tuple[Optional[str], float]:
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    analyzer = get_emotion_analyzer()
    return analyzer.get_emotion_state()

def reset_emotion_state() -> None:
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    analyzer = get_emotion_analyzer()
    analyzer.reset_emotion_state()

def reset_face_tracking() -> None:
    """ì–¼êµ´ ì¶”ì  ë¦¬ì…‹"""
    analyzer = get_emotion_analyzer()
    analyzer.reset_face_tracking()


# === ìƒˆë¡œìš´ í†µí•© í•¨ìˆ˜ë“¤ ===

def create_unified_analyzer(use_tracking: bool = True) -> EmotionAnalyzer:
    """í†µí•© ë¶„ì„ê¸° ìƒì„±"""
    analyzer = get_emotion_analyzer()
    if not use_tracking:
        analyzer.reset_face_tracking()
    return analyzer

def batch_analyze_emotions(images: list) -> list:
    """ë°°ì¹˜ ê°ì • ë¶„ì„"""
    analyzer = get_emotion_analyzer()
    results = []
    
    for img in images:
        emotion, confidence = analyzer.analyze_emotion(img)
        results.append((emotion, confidence))
    
    return results


# === ë””ë²„ê¹… ë° ìƒíƒœ í™•ì¸ í•¨ìˆ˜ë“¤ ===

def get_model_status() -> dict:
    """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
    manager = get_model_manager()
    return {
        'emotion_model_loaded': manager.emotion_pipeline is not None,
        'face_cascade_loaded': manager.face_cascade is not None,
        'model_id': manager._model_id,
        'device': manager._device
    }

def print_model_info():
    """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
    status = get_model_status()
    print("=" * 50)
    print("ğŸ” ëª¨ë¸ ìƒíƒœ ì •ë³´")
    print("=" * 50)
    print(f"ê°ì • ë¶„ì„ ëª¨ë¸: {'âœ… ë¡œë“œë¨' if status['emotion_model_loaded'] else 'âŒ ë¡œë“œ ì•ˆë¨'}")
    print(f"ì–¼êµ´ ê°ì§€ê¸°: {'âœ… ë¡œë“œë¨' if status['face_cascade_loaded'] else 'âŒ ë¡œë“œ ì•ˆë¨'}")
    print(f"ëª¨ë¸ ID: {status['model_id']}")

    # ğŸ”§ ì—¬ê¸° ê³ ì¹¨: ì¤‘ì²© f-string ì œê±°
    device_str = "CPU" if status["device"] == -1 else f"GPU {status['device']}"
    print(f"ë””ë°”ì´ìŠ¤: {device_str}")

    print("=" * 50)



if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print_model_info()
    
    try:
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ë¶„ì„ í…ŒìŠ¤íŠ¸
        test_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        emotion, confidence = analyze_emotion_from_image(test_img)
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {emotion} ({confidence:.2f})")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")