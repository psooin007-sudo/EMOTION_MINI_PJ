# web_with_graphs.py - ê°œì„ ëœ ë²„ì „
import cv2
import numpy as np
from PIL import Image
from transformers import pipeline, AutoImageProcessor
import webbrowser
import urllib.parse
import time
import json
import os
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from collections import defaultdict, Counter
from functools import lru_cache
import threading

# ì„¤ì •
MODEL_ID = "dima806/facial_emotions_image_detection"
DEVICE = -1
CAM_INDEX = 0
WIDTH, HEIGHT = 640, 480

# ë°°í¬ëœ Streamlit ì•± URL
DEPLOYED_APP_URL = "https://emotiondetector0827.streamlit.app/"

# ìˆ˜ì •ëœ ê°ì • ë§¤í•‘
EMOTION_MAPPING = {
    "sadness": "sad", 
    "happiness": "happy", 
    "anger": "angry",
    "fearful": "fear", 
    "surprised": "surprise", 
    "disgusted": "disgust",
    "neutral": "neutral"
}

# ê°ì •ë³„ ìƒ‰ìƒ (BGR for OpenCV, RGB for matplotlib)
EMOTION_COLORS = {
    "angry": (0, 0, 255),
    "sad": (255, 0, 0), 
    "happy": (0, 255, 0),
    "fear": (0, 165, 255),
    "surprise": (255, 255, 0),
    "disgust": (128, 0, 128),
    "neutral": (128, 128, 128)
}

# matplotlibìš© ìƒ‰ìƒ (RGB ì •ê·œí™”)
EMOTION_COLORS_MPL = {
    "angry": (1.0, 0.0, 0.0),
    "sad": (0.0, 0.0, 1.0), 
    "happy": (0.0, 1.0, 0.0),
    "fear": (1.0, 0.65, 0.0),
    "surprise": (1.0, 1.0, 0.0),
    "disgust": (0.5, 0.0, 0.5),
    "neutral": (0.5, 0.5, 0.5)
}


class FaceTracker:
    """ì•ˆì •ì ì¸ ë‹¨ì¼ ì–¼êµ´ ì¶”ì ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, max_missing_frames=10, min_overlap_ratio=0.3):
        self.tracked_face = None
        self.missing_frames = 0
        self.max_missing_frames = max_missing_frames
        self.min_overlap_ratio = min_overlap_ratio
        self.face_history = []  # ìµœê·¼ ì–¼êµ´ ìœ„ì¹˜ ê¸°ë¡
        self.max_history = 5
        
    def calculate_overlap_ratio(self, rect1, rect2):
        """ë‘ ì‚¬ê°í˜•ì˜ ê²¹ì¹˜ëŠ” ë¹„ìœ¨ ê³„ì‚°"""
        x1_1, y1_1, w1, h1 = rect1
        x2_1, y2_1 = x1_1 + w1, y1_1 + h1
        
        x1_2, y1_2, w2, h2 = rect2
        x2_2, y2_2 = x1_2 + w2, y1_2 + h2
        
        # ê²¹ì¹˜ëŠ” ì˜ì—­ ê³„ì‚°
        overlap_x1 = max(x1_1, x1_2)
        overlap_y1 = max(y1_1, y1_2)
        overlap_x2 = min(x2_1, x2_2)
        overlap_y2 = min(y2_1, y2_2)
        
        if overlap_x1 >= overlap_x2 or overlap_y1 >= overlap_y2:
            return 0.0
        
        overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
        rect1_area = w1 * h1
        
        return overlap_area / rect1_area if rect1_area > 0 else 0.0
    
    def find_best_face(self, faces):
        """ê°€ì¥ ì í•©í•œ ì–¼êµ´ ì„ íƒ (í¬ê¸°ì™€ ì¶”ì  ì¼ê´€ì„± ê³ ë ¤)"""
        if len(faces) == 0:
            return None
        
        # ì¶”ì  ì¤‘ì¸ ì–¼êµ´ì´ ì—†ìœ¼ë©´ ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
        if self.tracked_face is None:
            return max(faces, key=lambda x: x[2] * x[3])
        
        # ê¸°ì¡´ ì¶”ì  ì–¼êµ´ê³¼ ê²¹ì¹˜ëŠ” ì–¼êµ´ë“¤ ì°¾ê¸°
        matching_faces = []
        for face in faces:
            overlap = self.calculate_overlap_ratio(self.tracked_face, face)
            if overlap >= self.min_overlap_ratio:
                matching_faces.append((face, overlap))
        
        if matching_faces:
            # ê²¹ì¹˜ëŠ” ë¹„ìœ¨ì´ ê°€ì¥ ë†’ì€ ì–¼êµ´ ì„ íƒ
            return max(matching_faces, key=lambda x: x[1])[0]
        else:
            # ê²¹ì¹˜ëŠ” ì–¼êµ´ì´ ì—†ìœ¼ë©´ ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
            return max(faces, key=lambda x: x[2] * x[3])
    
    def update_tracking(self, faces):
        """ì–¼êµ´ ì¶”ì  ì—…ë°ì´íŠ¸ - ê°€ì¥ ì í•©í•œ ë‹¨ì¼ ì–¼êµ´ë§Œ ë°˜í™˜"""
        if len(faces) == 0:
            self.missing_frames += 1
            if self.missing_frames > self.max_missing_frames:
                self.tracked_face = None
                self.face_history.clear()
            return self.tracked_face
        
        # ê°€ì¥ ì í•©í•œ ì–¼êµ´ ì„ íƒ
        best_face = self.find_best_face(faces)
        
        if best_face is not None:
            self.tracked_face = best_face
            self.missing_frames = 0
            
            # ì–¼êµ´ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.face_history.append(best_face)
            if len(self.face_history) > self.max_history:
                self.face_history.pop(0)
        
        return self.tracked_face
    
    def get_stable_face(self):
        """ì•ˆì •í™”ëœ ì–¼êµ´ ìœ„ì¹˜ ë°˜í™˜ (ìµœê·¼ íˆìŠ¤í† ë¦¬ì˜ í‰ê· )"""
        if not self.face_history:
            return self.tracked_face
        
        # ìµœê·¼ ì–¼êµ´ ìœ„ì¹˜ë“¤ì˜ í‰ê· ìœ¼ë¡œ ì•ˆì •í™”
        avg_x = sum(face[0] for face in self.face_history) // len(self.face_history)
        avg_y = sum(face[1] for face in self.face_history) // len(self.face_history)
        avg_w = sum(face[2] for face in self.face_history) // len(self.face_history)
        avg_h = sum(face[3] for face in self.face_history) // len(self.face_history)
        
        return (avg_x, avg_y, avg_w, avg_h)
    
    def reset(self):
        """íŠ¸ë˜ì»¤ ë¦¬ì…‹"""
        self.tracked_face = None
        self.missing_frames = 0
        self.face_history.clear()


class ModelManager:
    """ëª¨ë¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ì‹±ê¸€í†¤ í´ë˜ìŠ¤ - ì¤‘ë³µ ë¡œë”© ë°©ì§€"""
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.emotion_pipeline = None
        self.face_cascade = None
        self._initialized = True
    
    @lru_cache(maxsize=1)
    def get_emotion_pipeline(self):
        """ê°ì • ë¶„ì„ ëª¨ë¸ì„ ìºì‹œí•˜ì—¬ í•œ ë²ˆë§Œ ë¡œë“œ"""
        if self.emotion_pipeline is None:
            try:
                print("ğŸ”„ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
                processor = AutoImageProcessor.from_pretrained(MODEL_ID, use_fast=True)
                self.emotion_pipeline = pipeline(
                    "image-classification", 
                    model=MODEL_ID,
                    image_processor=processor, 
                    device=DEVICE
                )
                print("âœ… ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                
                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
                test_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
                results = self.emotion_pipeline(test_img)
                print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {results[0]['label'] if results else 'No result'}")
                
            except Exception as e:
                print(f"âŒ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return self.emotion_pipeline
    
    @lru_cache(maxsize=1)
    def get_face_cascade(self):
        """ì–¼êµ´ ê°ì§€ê¸°ë¥¼ ìºì‹œí•˜ì—¬ í•œ ë²ˆë§Œ ë¡œë“œ"""
        if self.face_cascade is None:
            try:
                print("ğŸ”„ ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì¤‘...")
                self.face_cascade = cv2.CascadeClassifier(
                    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                )
                
                if self.face_cascade.empty():
                    raise ValueError("ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì‹¤íŒ¨")
                
                print("âœ… ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return self.face_cascade


class EmotionHistory:
    """ê°ì • íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ (ê·¸ë˜í”„ ê¸°ëŠ¥ í¬í•¨)"""
    def __init__(self, history_file="emotion_history.json"):
        self.history_file = history_file
        self.history = []  # [{timestamp, emotion, score, raw_emotion}]
        self.load_history()
    
    def load_history(self):
        """ì €ì¥ëœ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    self.history = json.load(f)
                print(f"ğŸ“š ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(self.history)}ê°œ ê¸°ë¡")
        except Exception as e:
            print(f"âš ï¸ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.history = []
    
    def add_record(self, emotion, score, raw_emotion=None):
        """ìƒˆ ê°ì • ê¸°ë¡ ì¶”ê°€"""
        record = {
            "timestamp": time.time(),
            "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "emotion": emotion,
            "score": float(score),
            "raw_emotion": raw_emotion or emotion
        }
        self.history.append(record)
        self.save_history()
        print(f"ğŸ“ íˆìŠ¤í† ë¦¬ ì¶”ê°€: {emotion} ({score*100:.1f}%)")
    
    def save_history(self):
        """íˆìŠ¤í† ë¦¬ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_recent_records(self, minutes=10):
        """ìµœê·¼ Në¶„ê°„ì˜ ê¸°ë¡ ë°˜í™˜"""
        cutoff_time = time.time() - (minutes * 60)
        return [r for r in self.history if r["timestamp"] > cutoff_time]
    
    def get_emotion_stats(self, minutes=None):
        """ê°ì •ë³„ í†µê³„ (ì „ì²´ ë˜ëŠ” ìµœê·¼ Në¶„)"""
        records = self.get_recent_records(minutes) if minutes else self.history
        stats = defaultdict(list)
        
        for record in records:
            stats[record["emotion"]].append(record["score"])
        
        # í‰ê·  ìŠ¤ì½”ì–´ì™€ íšŸìˆ˜ ê³„ì‚°
        result = {}
        for emotion, scores in stats.items():
            result[emotion] = {
                "count": len(scores),
                "avg_score": np.mean(scores) if scores else 0,
                "max_score": max(scores) if scores else 0,
                "min_score": min(scores) if scores else 0
            }
        
        return result
    
    def print_summary(self, minutes=10):
        """íˆìŠ¤í† ë¦¬ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š ê°ì • íˆìŠ¤í† ë¦¬ ìš”ì•½ (ìµœê·¼ {minutes}ë¶„)")
        print("=" * 50)
        
        recent = self.get_recent_records(minutes)
        if not recent:
            print("ğŸ“­ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        stats = self.get_emotion_stats(minutes)
        
        # ê°ì •ë³„ í†µê³„ ì¶œë ¥ (ë¹ˆë„ìˆœ ì •ë ¬)
        sorted_emotions = sorted(stats.items(), key=lambda x: x[1]["count"], reverse=True)
        
        print("ğŸ­ ê°ì •ë³„ í†µê³„:")
        for emotion, data in sorted_emotions:
            print(f"  {emotion.upper():8} | íšŸìˆ˜: {data['count']:2d} | "
                  f"í‰ê· : {data['avg_score']*100:5.1f}% | "
                  f"ìµœê³ : {data['max_score']*100:5.1f}%")
        
        # ìµœê·¼ 5ê°œ ê¸°ë¡
        print(f"\nâ±ï¸  ìµœê·¼ ê°ì • ë³€í™”:")
        for record in recent[-5:]:
            print(f"  {record['datetime'][:19]} | {record['emotion'].upper():8} | {record['score']*100:5.1f}%")


class WebcamEmotionAnalyzer:
    def __init__(self):
        # ì‹±ê¸€í†¤ ëª¨ë¸ ë§¤ë‹ˆì € ì‚¬ìš© - ì¤‘ë³µ ë¡œë”© ë°©ì§€
        self.model_manager = ModelManager()
        
        # íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ê°ì²´
        self.emotion_history = EmotionHistory()
        
        # ì–¼êµ´ íŠ¸ë˜ì»¤ - ë‹¨ì¼ ì–¼êµ´ ì•ˆì •ì  ì¶”ì 
        self.face_tracker = FaceTracker()
        
        # ì‹¤ì‹œê°„ í†µê³„ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        self.emotion_stats = {}
        
        print("ğŸ“¡ ëª¨ë¸ ì´ˆê¸°í™”...")
        self.load_models()
        
    def load_models(self):
        """ëª¨ë¸ ë¡œë“œ - ì´ì œ ì¤‘ë³µ ë¡œë”© ë°©ì§€"""
        # ëª¨ë¸ ë§¤ë‹ˆì €ë¥¼ í†µí•´ í•œ ë²ˆë§Œ ë¡œë“œ
        self.emotion_pipeline = self.model_manager.get_emotion_pipeline()
        self.face_cascade = self.model_manager.get_face_cascade()
        
        if self.emotion_pipeline and self.face_cascade:
            print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        else:
            print("âš ï¸ ì¼ë¶€ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    def detect_faces(self, frame):
        """ì–¼êµ´ ê²€ì¶œ - í–¥ìƒëœ íŒŒë¼ë¯¸í„°"""
        if self.face_cascade is None:
            return []
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ë” ì•ˆì •ì ì¸ ì–¼êµ´ ê²€ì¶œ ì„¤ì •
        faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.1,      # ë” ì •ë°€í•œ ìŠ¤ì¼€ì¼ íŒ©í„°
            minNeighbors=5,       # ë” ì—„ê²©í•œ í•„í„°ë§
            minSize=(80, 80),     # ë” í° ìµœì†Œ í¬ê¸°
            maxSize=(min(frame.shape[1]//2, 300), min(frame.shape[0]//2, 300)),  # ìµœëŒ€ í¬ê¸° ì œí•œ
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        # ì–¼êµ´ íŠ¸ë˜ì»¤ë¥¼ í†µí•´ ë‹¨ì¼ ì–¼êµ´ë§Œ ë°˜í™˜
        tracked_face = self.face_tracker.update_tracking(faces)
        
        if tracked_face is not None:
            return [tracked_face]  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì—¬ ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€
        else:
            return []
    
    def crop_and_preprocess_face(self, frame, faces):
        """ì–¼êµ´ í¬ë¡­ ë° ì „ì²˜ë¦¬ ê°œì„ """
        if len(faces) == 0:
            return None
        
        # ì•ˆì •í™”ëœ ì–¼êµ´ ìœ„ì¹˜ ì‚¬ìš©
        stable_face = self.face_tracker.get_stable_face()
        if stable_face is None:
            return None
            
        x, y, w, h = stable_face
        
        # ë” ì ì ˆí•œ ë§ˆì§„ìœ¼ë¡œ ì–¼êµ´ ì˜ì—­ í™•ì¥
        margin_x = int(w * 0.2)  # ê°€ë¡œ 20% ë§ˆì§„
        margin_y = int(h * 0.3)  # ì„¸ë¡œ 30% ë§ˆì§„ (ë¨¸ë¦¬ í¬í•¨)
        
        x1 = max(0, x - margin_x)
        y1 = max(0, y - margin_y)
        x2 = min(frame.shape[1], x + w + margin_x)
        y2 = min(frame.shape[0], y + h + margin_y)
        
        face_img = frame[y1:y2, x1:x2]
        
        # ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ 
        if face_img.shape[0] > 50 and face_img.shape[1] > 50:
            # í¬ê¸° ì •ê·œí™” (224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)
            face_img = cv2.resize(face_img, (224, 224))
            
            # ëŒ€ë¹„ ë° ë°ê¸° ê°œì„ 
            face_img = cv2.convertScaleAbs(face_img, alpha=1.1, beta=5)
            
        return face_img
    
    def analyze_emotion(self, face_img, save_to_history=True):
        """ê°ì • ë¶„ì„ (íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜µì…˜ ì¶”ê°€)"""
        if face_img is None or self.emotion_pipeline is None:
            return None, None
            
        try:
            # OpenCV BGRì„ RGBë¡œ ë³€í™˜
            rgb_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(rgb_img)
            
            # ê°ì • ë¶„ì„ ì‹¤í–‰
            results = self.emotion_pipeline(pil_img)
            
            if results and len(results) > 0:
                # ëª¨ë“  ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print("ğŸ­ ì „ì²´ ê°ì • ë¶„ì„ ê²°ê³¼:")
                for i, result in enumerate(results[:3]):  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                    raw_label = result["label"]
                    mapped_label = EMOTION_MAPPING.get(raw_label.lower(), raw_label.lower())
                    confidence = result["score"]
                    print(f"  {i+1}. {raw_label} -> {mapped_label}: {confidence*100:.2f}%")
                
                # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ê²°ê³¼ ì„ íƒ
                best_result = results[0]
                raw_emotion = best_result["label"]
                emotion = EMOTION_MAPPING.get(raw_emotion.lower(), raw_emotion.lower())
                score = float(best_result["score"])
                
                # ê¸°ì¡´ í†µê³„ ì—…ë°ì´íŠ¸
                if emotion in self.emotion_stats:
                    self.emotion_stats[emotion] += 1
                else:
                    self.emotion_stats[emotion] = 1
                
                # íˆìŠ¤í† ë¦¬ì— ì €ì¥
                if save_to_history:
                    self.emotion_history.add_record(emotion, score, raw_emotion)
                
                print(f"ğŸ¯ ì„ íƒëœ ê°ì •: {raw_emotion} -> {emotion} ({score*100:.1f}%)")
                print(f"ğŸ“Š ì‹¤ì‹œê°„ í†µê³„: {self.emotion_stats}")
                
                return emotion, score
                
        except Exception as e:
            print(f"âŒ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            
        return None, None
    
    def draw_face_info(self, frame, faces, emotion=None, score=None):
        """ì–¼êµ´ ë°•ìŠ¤ì™€ ê°ì • ì •ë³´ ê·¸ë¦¬ê¸° - ë‹¨ì¼ ì–¼êµ´ìš©"""
        if len(faces) == 0:
            return frame
        
        # ë‹¨ì¼ ì–¼êµ´ ì²˜ë¦¬
        face = faces[0]  # ì´ë¯¸ íŠ¸ë˜ì»¤ë¥¼ í†µí•´ ì„ íƒëœ ì–¼êµ´
        x, y, w, h = face
        
        color = EMOTION_COLORS.get(emotion, (0, 255, 0))
        
        # ì–¼êµ´ ë°•ìŠ¤ (ë” ë‘ê»ê²Œ)
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)
        
        # ê°ì • ë¼ë²¨
        if emotion and score is not None:
            # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ ì¡°ì •
            confidence_color = color if score > 0.6 else (128, 128, 128)
            
            label_text = f"{emotion.upper()} ({score*100:.1f}%)"
            label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
            
            # ë¼ë²¨ ë°°ê²½
            cv2.rectangle(frame, (x, y - 35), (x + label_size[0] + 15, y), confidence_color, -1)
            
            # ë¼ë²¨ í…ìŠ¤íŠ¸
            cv2.putText(frame, label_text, (x + 7, y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # íŠ¸ë˜í‚¹ ìƒíƒœ í‘œì‹œ
            status_text = f"Tracking: {self.face_tracker.missing_frames}/{self.face_tracker.max_missing_frames}"
            cv2.putText(frame, status_text, (x, y + h + 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        return frame
    
    def draw_history_info(self, frame, show_recent=True):
        """í™”ë©´ì— íˆìŠ¤í† ë¦¬ ì •ë³´ í‘œì‹œ"""
        if show_recent:
            recent_records = self.emotion_history.get_recent_records(5)  # ìµœê·¼ 5ë¶„
            if recent_records:
                # ìµœê·¼ ê°ì • ë³€í™” í‘œì‹œ
                y_start = 120
                cv2.putText(frame, "Recent Emotions:", (WIDTH - 200, y_start),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                
                for i, record in enumerate(recent_records[-3:]):  # ìµœê·¼ 3ê°œë§Œ
                    emotion = record["emotion"]
                    score = record["score"]
                    time_str = record["datetime"][-8:-3]  # HH:MMë§Œ
                    
                    color = EMOTION_COLORS.get(emotion, (255, 255, 255))
                    text = f"{time_str} {emotion[:4].upper()} {score*100:.0f}%"
                    
                    cv2.putText(frame, text, (WIDTH - 200, y_start + 20 + i*15),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        return frame
    
    def save_result_locally(self, emotion, score):
        """ê²°ê³¼ë¥¼ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥ (íˆìŠ¤í† ë¦¬ í¬í•¨)"""
        result = {
            "emotion": emotion,
            "score": score,
            "timestamp": time.time(),
            "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "emotion_stats": self.emotion_stats.copy(),
            "recent_history": self.emotion_history.get_recent_records(10),  # ìµœê·¼ 10ë¶„
            "total_records": len(self.emotion_history.history)
        }
        
        with open("latest_emotion_result.json", "w", encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    
    def run(self):
        """ë©”ì¸ ë£¨í”„ ì‹¤í–‰"""
        cap = cv2.VideoCapture(CAM_INDEX)
        if not cap.isOpened():
            raise RuntimeError("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)
        
        # ì¹´ë©”ë¼ ì„¤ì • ê°œì„ 
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_BRIGHTNESS, 0.5)
        cap.set(cv2.CAP_PROP_CONTRAST, 0.5)
        
        window_name = "Real-time Emotion Analysis (Single Face Tracking)"
        cv2.namedWindow(window_name)
        
        last_emotion, last_score = None, None
        frame_count = 0
        show_history = True
        
        print("ğŸ¥ ì›¹ìº  ì‹œì‘! (ë‹¨ì¼ ì–¼êµ´ ì¶”ì  ëª¨ë“œ)")
        print(f"ğŸŒ ê²°ê³¼ í˜ì´ì§€: {DEPLOYED_APP_URL}")
        print("ğŸ’¡ íŒ: ì¹´ë©”ë¼ ì•ì—ì„œ ë‹¤ì–‘í•œ í‘œì •ì„ ì§€ì–´ë³´ì„¸ìš”!")
        print("âŒ¨ï¸  ì¡°ì‘ë²•:")
        print("  ESC: ì¢…ë£Œ")
        print("  R: ì–¼êµ´ íŠ¸ë˜í‚¹ ë¦¬ì…‹")
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                    
                frame_count += 1
                faces = self.detect_faces(frame)  # ì´ë¯¸ ë‹¨ì¼ ì–¼êµ´ë§Œ ë°˜í™˜
                
                # ê°ì • ë¶„ì„ (15í”„ë ˆì„ë§ˆë‹¤, íˆìŠ¤í† ë¦¬ëŠ” ëœ ìì£¼)
                if frame_count % 15 == 0 and len(faces) > 0:
                    face_img = self.crop_and_preprocess_face(frame, faces)
                    # íˆìŠ¤í† ë¦¬ ì €ì¥ì€ 45í”„ë ˆì„ë§ˆë‹¤ë§Œ (ë„ˆë¬´ ë§ì€ ë°ì´í„° ë°©ì§€)
                    save_history = frame_count % 45 == 0
                    emotion, score = self.analyze_emotion(face_img, save_to_history=save_history)
                    if emotion:
                        last_emotion = emotion
                        last_score = score
                        # ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥
                        self.save_result_locally(emotion, score)
                
                # ì–¼êµ´ ì •ë³´ í‘œì‹œ
                frame = self.draw_face_info(frame, faces, last_emotion, last_score)
                
                # íˆìŠ¤í† ë¦¬ ì •ë³´ í‘œì‹œ
                if show_history:
                    frame = self.draw_history_info(frame)
                
                # ìƒíƒœ ì •ë³´ í‘œì‹œ
                face_status = f"Face: {'âœ“' if len(faces) > 0 else 'âœ—'}"
                if len(faces) > 0:
                    missing = self.face_tracker.missing_frames
                    face_status += f" (Missing: {missing})"
                
                emotion_status = ""
                if last_emotion:
                    emotion_status = f"Emotion: {last_emotion.upper()} ({(last_score or 0)*100:.1f}%)"
                else:
                    emotion_status = "Detecting emotions..."
                
                status = f"{face_status} | {emotion_status} | Records: {len(self.emotion_history.history)}"
                
                cv2.putText(frame, status, (10, HEIGHT - 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # ì‚¬ìš©ë²• ì•ˆë‚´
                help_text = "Single Face Tracking | ESC: quit | R: reset tracking"
                cv2.putText(frame, help_text, (10, HEIGHT - 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
                
                cv2.imshow(window_name, frame)
                
                # í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC í‚¤
                    print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                    break
                elif key == ord('r') or key == ord('R'):  # R í‚¤ë¡œ íŠ¸ë˜í‚¹ ë¦¬ì…‹
                    print("ğŸ”„ ì–¼êµ´ íŠ¸ë˜í‚¹ ë¦¬ì…‹")
                    self.face_tracker.reset()
                    last_emotion, last_score = None, None
                    
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„ ì¶œë ¥
            print("\nğŸ ì„¸ì…˜ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ê¸°ë¡ëœ ê°ì •: {len(self.emotion_history.history)}ê°œ")
            print(f"ğŸ­ ì‹¤ì‹œê°„ í†µê³„: {self.emotion_stats}")
            self.emotion_history.print_summary(60)  # ìµœê·¼ 1ì‹œê°„


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ­ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ê¸° (ê°œì„ ëœ ë²„ì „)")
    print("=" * 60)
    print("âœ¨ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("  - ëª¨ë¸ ì¤‘ë³µ ë¡œë”© ë°©ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)")
    print("  - ë‹¨ì¼ ì–¼êµ´ ì•ˆì •ì  ì¶”ì ")
    print("  - í–¥ìƒëœ ì–¼êµ´ ì¸ì‹ ì •í™•ë„")
    print("  - íŠ¸ë˜í‚¹ ìƒíƒœ ì‹œê°í™”")
    print()
    print(f"ğŸŒ ê²°ê³¼ í˜ì´ì§€: {DEPLOYED_APP_URL}")
    print("ğŸ’¡ ë‹¤ì–‘í•œ ê°ì • í‘œí˜„ì„ ì‹œë„í•´ë³´ì„¸ìš”!")
    print("=" * 60)
    
    try:
        analyzer = WebcamEmotionAnalyzer()
        analyzer.run()
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ğŸ”š í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    main()