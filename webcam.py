<<<<<<< HEAD
# web_with_graphs.py - ê°œì„ ëœ ë²„ì „
=======
# web_with_graphs.py
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
import cv2
import numpy as np
from PIL import Image
from transformers import pipeline, AutoImageProcessor
import webbrowser
import urllib.parse
import time
import json
import os
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from collections import defaultdict, Counter
<<<<<<< HEAD
from functools import lru_cache
import threading
=======
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760

# ì„¤ì •
MODEL_ID = "dima806/facial_emotions_image_detection"
DEVICE = -1
CAM_INDEX = 0
WIDTH, HEIGHT = 640, 480

# ë°°í¬ëœ Streamlit ì•± URL
DEPLOYED_APP_URL = "https://emotiondetector0827.streamlit.app/"

<<<<<<< HEAD
# ìˆ˜ì •ëœ ê°ì • ë§¤í•‘
=======
# ìˆ˜ì •ëœ ê°ì • ë§¤í•‘ (disgustë¥¼ ë‹¤ì‹œ í™œì„±í™”)
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
EMOTION_MAPPING = {
    "sadness": "sad", 
    "happiness": "happy", 
    "anger": "angry",
    "fearful": "fear", 
    "surprised": "surprise", 
<<<<<<< HEAD
    "disgusted": "disgust",
=======
    "disgusted": "disgust",  # ë‹¤ì‹œ disgustë¡œ ë§¤í•‘
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
    "neutral": "neutral"
}

# ê°ì •ë³„ ìƒ‰ìƒ (BGR for OpenCV, RGB for matplotlib)
EMOTION_COLORS = {
    "angry": (0, 0, 255),
    "sad": (255, 0, 0), 
    "happy": (0, 255, 0),
    "fear": (0, 165, 255),
    "surprise": (255, 255, 0),
<<<<<<< HEAD
    "disgust": (128, 0, 128),
=======
    "disgust": (128, 0, 128),  # disgust ìƒ‰ìƒ ì¶”ê°€
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
    "neutral": (128, 128, 128)
}

# matplotlibìš© ìƒ‰ìƒ (RGB ì •ê·œí™”)
EMOTION_COLORS_MPL = {
    "angry": (1.0, 0.0, 0.0),
    "sad": (0.0, 0.0, 1.0), 
    "happy": (0.0, 1.0, 0.0),
    "fear": (1.0, 0.65, 0.0),
    "surprise": (1.0, 1.0, 0.0),
    "disgust": (0.5, 0.0, 0.5),
    "neutral": (0.5, 0.5, 0.5)
}

<<<<<<< HEAD

class FaceTracker:
    """ì•ˆì •ì ì¸ ë‹¨ì¼ ì–¼êµ´ ì¶”ì ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, max_missing_frames=10, min_overlap_ratio=0.3):
        self.tracked_face = None
        self.missing_frames = 0
        self.max_missing_frames = max_missing_frames
        self.min_overlap_ratio = min_overlap_ratio
        self.face_history = []  # ìµœê·¼ ì–¼êµ´ ìœ„ì¹˜ ê¸°ë¡
        self.max_history = 5
        
    def calculate_overlap_ratio(self, rect1, rect2):
        """ë‘ ì‚¬ê°í˜•ì˜ ê²¹ì¹˜ëŠ” ë¹„ìœ¨ ê³„ì‚°"""
        x1_1, y1_1, w1, h1 = rect1
        x2_1, y2_1 = x1_1 + w1, y1_1 + h1
        
        x1_2, y1_2, w2, h2 = rect2
        x2_2, y2_2 = x1_2 + w2, y1_2 + h2
        
        # ê²¹ì¹˜ëŠ” ì˜ì—­ ê³„ì‚°
        overlap_x1 = max(x1_1, x1_2)
        overlap_y1 = max(y1_1, y1_2)
        overlap_x2 = min(x2_1, x2_2)
        overlap_y2 = min(y2_1, y2_2)
        
        if overlap_x1 >= overlap_x2 or overlap_y1 >= overlap_y2:
            return 0.0
        
        overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
        rect1_area = w1 * h1
        
        return overlap_area / rect1_area if rect1_area > 0 else 0.0
    
    def find_best_face(self, faces):
        """ê°€ì¥ ì í•©í•œ ì–¼êµ´ ì„ íƒ (í¬ê¸°ì™€ ì¶”ì  ì¼ê´€ì„± ê³ ë ¤)"""
        if len(faces) == 0:
            return None
        
        # ì¶”ì  ì¤‘ì¸ ì–¼êµ´ì´ ì—†ìœ¼ë©´ ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
        if self.tracked_face is None:
            return max(faces, key=lambda x: x[2] * x[3])
        
        # ê¸°ì¡´ ì¶”ì  ì–¼êµ´ê³¼ ê²¹ì¹˜ëŠ” ì–¼êµ´ë“¤ ì°¾ê¸°
        matching_faces = []
        for face in faces:
            overlap = self.calculate_overlap_ratio(self.tracked_face, face)
            if overlap >= self.min_overlap_ratio:
                matching_faces.append((face, overlap))
        
        if matching_faces:
            # ê²¹ì¹˜ëŠ” ë¹„ìœ¨ì´ ê°€ì¥ ë†’ì€ ì–¼êµ´ ì„ íƒ
            return max(matching_faces, key=lambda x: x[1])[0]
        else:
            # ê²¹ì¹˜ëŠ” ì–¼êµ´ì´ ì—†ìœ¼ë©´ ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
            return max(faces, key=lambda x: x[2] * x[3])
    
    def update_tracking(self, faces):
        """ì–¼êµ´ ì¶”ì  ì—…ë°ì´íŠ¸ - ê°€ì¥ ì í•©í•œ ë‹¨ì¼ ì–¼êµ´ë§Œ ë°˜í™˜"""
        if len(faces) == 0:
            self.missing_frames += 1
            if self.missing_frames > self.max_missing_frames:
                self.tracked_face = None
                self.face_history.clear()
            return self.tracked_face
        
        # ê°€ì¥ ì í•©í•œ ì–¼êµ´ ì„ íƒ
        best_face = self.find_best_face(faces)
        
        if best_face is not None:
            self.tracked_face = best_face
            self.missing_frames = 0
            
            # ì–¼êµ´ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.face_history.append(best_face)
            if len(self.face_history) > self.max_history:
                self.face_history.pop(0)
        
        return self.tracked_face
    
    def get_stable_face(self):
        """ì•ˆì •í™”ëœ ì–¼êµ´ ìœ„ì¹˜ ë°˜í™˜ (ìµœê·¼ íˆìŠ¤í† ë¦¬ì˜ í‰ê· )"""
        if not self.face_history:
            return self.tracked_face
        
        # ìµœê·¼ ì–¼êµ´ ìœ„ì¹˜ë“¤ì˜ í‰ê· ìœ¼ë¡œ ì•ˆì •í™”
        avg_x = sum(face[0] for face in self.face_history) // len(self.face_history)
        avg_y = sum(face[1] for face in self.face_history) // len(self.face_history)
        avg_w = sum(face[2] for face in self.face_history) // len(self.face_history)
        avg_h = sum(face[3] for face in self.face_history) // len(self.face_history)
        
        return (avg_x, avg_y, avg_w, avg_h)
    
    def reset(self):
        """íŠ¸ë˜ì»¤ ë¦¬ì…‹"""
        self.tracked_face = None
        self.missing_frames = 0
        self.face_history.clear()


class ModelManager:
    """ëª¨ë¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ì‹±ê¸€í†¤ í´ë˜ìŠ¤ - ì¤‘ë³µ ë¡œë”© ë°©ì§€"""
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.emotion_pipeline = None
        self.face_cascade = None
        self._initialized = True
    
    @lru_cache(maxsize=1)
    def get_emotion_pipeline(self):
        """ê°ì • ë¶„ì„ ëª¨ë¸ì„ ìºì‹œí•˜ì—¬ í•œ ë²ˆë§Œ ë¡œë“œ"""
        if self.emotion_pipeline is None:
            try:
                print("ğŸ”„ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
                processor = AutoImageProcessor.from_pretrained(MODEL_ID, use_fast=True)
                self.emotion_pipeline = pipeline(
                    "image-classification", 
                    model=MODEL_ID,
                    image_processor=processor, 
                    device=DEVICE
                )
                print("âœ… ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                
                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
                test_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
                results = self.emotion_pipeline(test_img)
                print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {results[0]['label'] if results else 'No result'}")
                
            except Exception as e:
                print(f"âŒ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return self.emotion_pipeline
    
    @lru_cache(maxsize=1)
    def get_face_cascade(self):
        """ì–¼êµ´ ê°ì§€ê¸°ë¥¼ ìºì‹œí•˜ì—¬ í•œ ë²ˆë§Œ ë¡œë“œ"""
        if self.face_cascade is None:
            try:
                print("ğŸ”„ ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì¤‘...")
                self.face_cascade = cv2.CascadeClassifier(
                    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                )
                
                if self.face_cascade.empty():
                    raise ValueError("ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì‹¤íŒ¨")
                
                print("âœ… ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ ì–¼êµ´ ê°ì§€ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return self.face_cascade


=======
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
class EmotionHistory:
    """ê°ì • íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ (ê·¸ë˜í”„ ê¸°ëŠ¥ í¬í•¨)"""
    def __init__(self, history_file="emotion_history.json"):
        self.history_file = history_file
        self.history = []  # [{timestamp, emotion, score, raw_emotion}]
        self.load_history()
    
    def load_history(self):
        """ì €ì¥ëœ íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    self.history = json.load(f)
                print(f"ğŸ“š ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(self.history)}ê°œ ê¸°ë¡")
        except Exception as e:
            print(f"âš ï¸ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.history = []
    
    def add_record(self, emotion, score, raw_emotion=None):
        """ìƒˆ ê°ì • ê¸°ë¡ ì¶”ê°€"""
        record = {
            "timestamp": time.time(),
            "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "emotion": emotion,
            "score": float(score),
            "raw_emotion": raw_emotion or emotion
        }
        self.history.append(record)
        self.save_history()
        print(f"ğŸ“ íˆìŠ¤í† ë¦¬ ì¶”ê°€: {emotion} ({score*100:.1f}%)")
    
    def save_history(self):
        """íˆìŠ¤í† ë¦¬ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_recent_records(self, minutes=10):
        """ìµœê·¼ Në¶„ê°„ì˜ ê¸°ë¡ ë°˜í™˜"""
        cutoff_time = time.time() - (minutes * 60)
        return [r for r in self.history if r["timestamp"] > cutoff_time]
    
    def get_emotion_stats(self, minutes=None):
        """ê°ì •ë³„ í†µê³„ (ì „ì²´ ë˜ëŠ” ìµœê·¼ Në¶„)"""
        records = self.get_recent_records(minutes) if minutes else self.history
        stats = defaultdict(list)
        
        for record in records:
            stats[record["emotion"]].append(record["score"])
        
        # í‰ê·  ìŠ¤ì½”ì–´ì™€ íšŸìˆ˜ ê³„ì‚°
        result = {}
        for emotion, scores in stats.items():
            result[emotion] = {
                "count": len(scores),
                "avg_score": np.mean(scores) if scores else 0,
                "max_score": max(scores) if scores else 0,
                "min_score": min(scores) if scores else 0
            }
        
        return result
    
<<<<<<< HEAD
=======
    def get_timeline_data(self, minutes=60):
        """ì‹œê°„ëŒ€ë³„ ê°ì • ë³€í™” ë°ì´í„°"""
        recent_records = self.get_recent_records(minutes)
        return [(r["datetime"], r["emotion"], r["score"]) for r in recent_records]
    
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
    def print_summary(self, minutes=10):
        """íˆìŠ¤í† ë¦¬ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š ê°ì • íˆìŠ¤í† ë¦¬ ìš”ì•½ (ìµœê·¼ {minutes}ë¶„)")
        print("=" * 50)
        
        recent = self.get_recent_records(minutes)
        if not recent:
            print("ğŸ“­ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        stats = self.get_emotion_stats(minutes)
        
        # ê°ì •ë³„ í†µê³„ ì¶œë ¥ (ë¹ˆë„ìˆœ ì •ë ¬)
        sorted_emotions = sorted(stats.items(), key=lambda x: x[1]["count"], reverse=True)
        
        print("ğŸ­ ê°ì •ë³„ í†µê³„:")
        for emotion, data in sorted_emotions:
            print(f"  {emotion.upper():8} | íšŸìˆ˜: {data['count']:2d} | "
                  f"í‰ê· : {data['avg_score']*100:5.1f}% | "
                  f"ìµœê³ : {data['max_score']*100:5.1f}%")
        
        # ìµœê·¼ 5ê°œ ê¸°ë¡
        print(f"\nâ±ï¸  ìµœê·¼ ê°ì • ë³€í™”:")
        for record in recent[-5:]:
            print(f"  {record['datetime'][:19]} | {record['emotion'].upper():8} | {record['score']*100:5.1f}%")
<<<<<<< HEAD
=======
    
    def export_csv(self, filename="emotion_history.csv"):
        """CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            import pandas as pd
            df = pd.DataFrame(self.history)
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"ğŸ“„ íˆìŠ¤í† ë¦¬ë¥¼ {filename}ìœ¼ë¡œ ë‚´ë³´ëƒ„ ({len(self.history)}ê°œ ê¸°ë¡)")
        except ImportError:
            # pandas ì—†ì„ ë•Œ ìˆ˜ë™ìœ¼ë¡œ CSV ìƒì„±
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write("timestamp,datetime,emotion,score,raw_emotion\n")
                    for record in self.history:
                        f.write(f"{record['timestamp']},{record['datetime']},{record['emotion']},{record['score']},{record['raw_emotion']}\n")
                print(f"ğŸ“„ íˆìŠ¤í† ë¦¬ë¥¼ {filename}ìœ¼ë¡œ ë‚´ë³´ëƒ„ ({len(self.history)}ê°œ ê¸°ë¡)")
            except Exception as e:
                print(f"âŒ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"âŒ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

    def plot_emotion_timeline(self, minutes=60, save_file=None):
        """ê°ì • ë³€í™” íƒ€ì„ë¼ì¸ ê·¸ë˜í”„"""
        recent_records = self.get_recent_records(minutes)
        
        if not recent_records:
            print("ğŸ“­ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° ì¤€ë¹„
        timestamps = [datetime.fromtimestamp(r["timestamp"]) for r in recent_records]
        emotions = [r["emotion"] for r in recent_records]
        scores = [r["score"] for r in recent_records]
        
        # ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(12, 8))
        
        # ì„œë¸Œí”Œë¡¯ 1: ê°ì •ë³„ ìƒ‰ìƒìœ¼ë¡œ ì‹œê°„ ë³€í™”
        plt.subplot(2, 1, 1)
        
        for emotion in set(emotions):
            emotion_times = [timestamps[i] for i, e in enumerate(emotions) if e == emotion]
            emotion_scores = [scores[i] for i, e in enumerate(emotions) if e == emotion]
            
            plt.scatter(emotion_times, emotion_scores, 
                       c=[EMOTION_COLORS_MPL[emotion]], 
                       label=emotion.upper(), 
                       alpha=0.7, s=60)
        
        plt.title(f'ê°ì • ë³€í™” íƒ€ì„ë¼ì¸ (ìµœê·¼ {minutes}ë¶„)', fontsize=16, fontweight='bold')
        plt.xlabel('ì‹œê°„')
        plt.ylabel('ê°ì • ì‹ ë¢°ë„')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)
        
        # xì¶• ì‹œê°„ í¬ë§·
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        plt.gca().xaxis.set_major_locator(mdates.MinuteLocator(interval=5))
        plt.xticks(rotation=45)
        
        # ì„œë¸Œí”Œë¡¯ 2: ê°ì •ë³„ ë¹ˆë„ ë§‰ëŒ€ ê·¸ë˜í”„
        plt.subplot(2, 1, 2)
        
        emotion_counts = Counter(emotions)
        emotions_list = list(emotion_counts.keys())
        counts = list(emotion_counts.values())
        colors = [EMOTION_COLORS_MPL[e] for e in emotions_list]
        
        bars = plt.bar(emotions_list, counts, color=colors, alpha=0.7)
        plt.title('ê°ì •ë³„ ë¹ˆë„', fontsize=14, fontweight='bold')
        plt.xlabel('ê°ì •')
        plt.ylabel('íšŸìˆ˜')
        
        # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
        for bar, count in zip(bars, counts):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    str(count), ha='center', va='bottom')
        
        plt.tight_layout()
        
        # íŒŒì¼ë¡œ ì €ì¥
        if save_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_file = f"emotion_timeline_{timestamp}.png"
        
        plt.savefig(save_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ ê·¸ë˜í”„ë¥¼ {save_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
        
        plt.show()
    
    def plot_emotion_heatmap(self, save_file=None):
        """ì‹œê°„ëŒ€ë³„ ê°ì • íˆíŠ¸ë§µ"""
        if not self.history:
            print("ğŸ“­ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì‹œê°„ëŒ€ë³„ ë°ì´í„° ì¤€ë¹„
        hours_data = defaultdict(lambda: defaultdict(int))
        
        for record in self.history:
            dt = datetime.fromtimestamp(record["timestamp"])
            hour = dt.hour
            emotion = record["emotion"]
            hours_data[hour][emotion] += 1
        
        # ëª¨ë“  ê°ì • ëª©ë¡
        all_emotions = list(EMOTION_MAPPING.values())
        all_hours = range(24)
        
        # íˆíŠ¸ë§µ ë°ì´í„° ìƒì„±
        heatmap_data = []
        for emotion in all_emotions:
            row = [hours_data[hour][emotion] for hour in all_hours]
            heatmap_data.append(row)
        
        # ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(15, 8))
        
        im = plt.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')
        
        # ì¶• ì„¤ì •
        plt.yticks(range(len(all_emotions)), [e.upper() for e in all_emotions])
        plt.xticks(range(24), [f'{h:02d}:00' for h in range(24)])
        
        # ì œëª©ê³¼ ë¼ë²¨
        plt.title('ì‹œê°„ëŒ€ë³„ ê°ì • ë¶„í¬ íˆíŠ¸ë§µ', fontsize=16, fontweight='bold')
        plt.xlabel('ì‹œê°„')
        plt.ylabel('ê°ì •')
        
        # ì»¬ëŸ¬ë°”
        cbar = plt.colorbar(im)
        cbar.set_label('ë¹ˆë„', rotation=270, labelpad=20)
        
        # ê°’ í‘œì‹œ
        for i in range(len(all_emotions)):
            for j in range(24):
                text = plt.text(j, i, heatmap_data[i][j], 
                               ha="center", va="center", color="black", fontsize=8)
        
        plt.tight_layout()
        
        # íŒŒì¼ë¡œ ì €ì¥
        if save_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_file = f"emotion_heatmap_{timestamp}.png"
        
        plt.savefig(save_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ”¥ íˆíŠ¸ë§µì„ {save_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
        
        plt.show()
    
    def plot_emotion_pie_chart(self, minutes=None, save_file=None):
        """ê°ì •ë³„ ë¹„ìœ¨ ì›í˜• ê·¸ë˜í”„"""
        records = self.get_recent_records(minutes) if minutes else self.history
        
        if not records:
            print("ğŸ“­ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê°ì •ë³„ ì¹´ìš´íŠ¸
        emotions = [r["emotion"] for r in records]
        emotion_counts = Counter(emotions)
        
        # ê·¸ë˜í”„ ë°ì´í„°
        labels = list(emotion_counts.keys())
        sizes = list(emotion_counts.values())
        colors = [EMOTION_COLORS_MPL[emotion] for emotion in labels]
        
        # ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(10, 8))
        
        wedges, texts, autotexts = plt.pie(sizes, labels=[l.upper() for l in labels], 
                                          colors=colors, autopct='%1.1f%%',
                                          startangle=90, textprops={'fontsize': 12})
        
        # ì œëª©
        period_text = f"ìµœê·¼ {minutes}ë¶„" if minutes else "ì „ì²´ ê¸°ê°„"
        plt.title(f'ê°ì • ë¶„í¬ ({period_text})', fontsize=16, fontweight='bold')
        
        # ë²”ë¡€
        plt.legend(wedges, [f"{l.upper()} ({c}íšŒ)" for l, c in zip(labels, sizes)],
                  title="ê°ì •ë³„ íšŸìˆ˜", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
        
        plt.axis('equal')
        
        # íŒŒì¼ë¡œ ì €ì¥
        if save_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            period_suffix = f"_{minutes}min" if minutes else "_total"
            save_file = f"emotion_pie{period_suffix}_{timestamp}.png"
        
        plt.savefig(save_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ¥§ ì›í˜• ê·¸ë˜í”„ë¥¼ {save_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
        
        plt.show()
    
    def plot_emotion_score_distribution(self, save_file=None):
        """ê°ì •ë³„ ì‹ ë¢°ë„ ë¶„í¬ ìƒì ê·¸ë˜í”„"""
        if not self.history:
            print("ğŸ“­ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê°ì •ë³„ ìŠ¤ì½”ì–´ ë°ì´í„° ì¤€ë¹„
        emotion_scores = defaultdict(list)
        for record in self.history:
            emotion_scores[record["emotion"]].append(record["score"])
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê°ì •ë§Œ í•„í„°ë§
        filtered_emotions = {k: v for k, v in emotion_scores.items() if v}
        
        if not filtered_emotions:
            print("ğŸ“­ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìŠ¤ì½”ì–´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(12, 8))
        
        emotions = list(filtered_emotions.keys())
        scores_data = list(filtered_emotions.values())
        
        # ìƒì ê·¸ë˜í”„
        box_plot = plt.boxplot(scores_data, labels=[e.upper() for e in emotions], patch_artist=True)
        
        # ìƒ‰ìƒ ì„¤ì •
        for patch, emotion in zip(box_plot['boxes'], emotions):
            patch.set_facecolor(EMOTION_COLORS_MPL[emotion])
            patch.set_alpha(0.7)
        
        plt.title('ê°ì •ë³„ ì‹ ë¢°ë„ ë¶„í¬', fontsize=16, fontweight='bold')
        plt.xlabel('ê°ì •')
        plt.ylabel('ì‹ ë¢°ë„')
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        stats_text = []
        for emotion in emotions:
            scores = filtered_emotions[emotion]
            avg_score = np.mean(scores)
            stats_text.append(f"{emotion.upper()}: í‰ê·  {avg_score:.2f}")
        
        plt.figtext(0.02, 0.02, " | ".join(stats_text), fontsize=10, alpha=0.7)
        
        plt.tight_layout()
        
        # íŒŒì¼ë¡œ ì €ì¥
        if save_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_file = f"emotion_score_dist_{timestamp}.png"
        
        plt.savefig(save_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“¦ ìƒì ê·¸ë˜í”„ë¥¼ {save_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
        
        plt.show()
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760


class WebcamEmotionAnalyzer:
    def __init__(self):
<<<<<<< HEAD
        # ì‹±ê¸€í†¤ ëª¨ë¸ ë§¤ë‹ˆì € ì‚¬ìš© - ì¤‘ë³µ ë¡œë”© ë°©ì§€
        self.model_manager = ModelManager()
=======
        self.emotion_pipeline = None
        self.face_cascade = None
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
        
        # íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ê°ì²´
        self.emotion_history = EmotionHistory()
        
<<<<<<< HEAD
        # ì–¼êµ´ íŠ¸ë˜ì»¤ - ë‹¨ì¼ ì–¼êµ´ ì•ˆì •ì  ì¶”ì 
        self.face_tracker = FaceTracker()
        
        # ì‹¤ì‹œê°„ í†µê³„ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        self.emotion_stats = {}
        
        print("ğŸ“¡ ëª¨ë¸ ì´ˆê¸°í™”...")
        self.load_models()
        
    def load_models(self):
        """ëª¨ë¸ ë¡œë“œ - ì´ì œ ì¤‘ë³µ ë¡œë”© ë°©ì§€"""
        # ëª¨ë¸ ë§¤ë‹ˆì €ë¥¼ í†µí•´ í•œ ë²ˆë§Œ ë¡œë“œ
        self.emotion_pipeline = self.model_manager.get_emotion_pipeline()
        self.face_cascade = self.model_manager.get_face_cascade()
        
        if self.emotion_pipeline and self.face_cascade:
            print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        else:
            print("âš ï¸ ì¼ë¶€ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    def detect_faces(self, frame):
        """ì–¼êµ´ ê²€ì¶œ - í–¥ìƒëœ íŒŒë¼ë¯¸í„°"""
        if self.face_cascade is None:
            return []
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ë” ì•ˆì •ì ì¸ ì–¼êµ´ ê²€ì¶œ ì„¤ì •
        faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.1,      # ë” ì •ë°€í•œ ìŠ¤ì¼€ì¼ íŒ©í„°
            minNeighbors=5,       # ë” ì—„ê²©í•œ í•„í„°ë§
            minSize=(80, 80),     # ë” í° ìµœì†Œ í¬ê¸°
            maxSize=(min(frame.shape[1]//2, 300), min(frame.shape[0]//2, 300)),  # ìµœëŒ€ í¬ê¸° ì œí•œ
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        # ì–¼êµ´ íŠ¸ë˜ì»¤ë¥¼ í†µí•´ ë‹¨ì¼ ì–¼êµ´ë§Œ ë°˜í™˜
        tracked_face = self.face_tracker.update_tracking(faces)
        
        if tracked_face is not None:
            return [tracked_face]  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì—¬ ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€
        else:
            return []
=======
        # ì‹¤ì‹œê°„ í†µê³„ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        self.emotion_stats = {}
        
        self.load_models()
        
    def load_models(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("Loading models...")
        
        # ì–¼êµ´ ê²€ì¶œê¸°
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # ê°ì • ë¶„ì„ ëª¨ë¸
        try:
            print("Loading emotion classification model...")
            processor = AutoImageProcessor.from_pretrained(MODEL_ID, use_fast=True)
            self.emotion_pipeline = pipeline(
                "image-classification", 
                model=MODEL_ID,
                image_processor=processor, 
                device=DEVICE
            )
            print("âœ… Models loaded successfully!")
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            self.test_model()
            
        except Exception as e:
            print(f"âŒ Model loading error: {e}")
            
    def test_model(self):
        """ëª¨ë¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        try:
            # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            test_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
            results = self.emotion_pipeline(test_img)
            print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {results}")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
    def detect_faces(self, frame):
        """ì–¼êµ´ ê²€ì¶œ (ë” ë¯¼ê°í•˜ê²Œ ì¡°ì •)"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ë” ë¯¼ê°í•œ ì–¼êµ´ ê²€ì¶œ ì„¤ì •
        faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.05,     # ë” ì‘ì€ ìŠ¤ì¼€ì¼ íŒ©í„°
            minNeighbors=3,       # ë” ì ì€ ì´ì›ƒ ìˆ˜
            minSize=(50, 50),     # ë” í° ìµœì†Œ í¬ê¸°
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        return faces
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
    
    def crop_and_preprocess_face(self, frame, faces):
        """ì–¼êµ´ í¬ë¡­ ë° ì „ì²˜ë¦¬ ê°œì„ """
        if len(faces) == 0:
            return None
<<<<<<< HEAD
        
        # ì•ˆì •í™”ëœ ì–¼êµ´ ìœ„ì¹˜ ì‚¬ìš©
        stable_face = self.face_tracker.get_stable_face()
        if stable_face is None:
            return None
            
        x, y, w, h = stable_face
        
        # ë” ì ì ˆí•œ ë§ˆì§„ìœ¼ë¡œ ì–¼êµ´ ì˜ì—­ í™•ì¥
=======
            
        largest_face = max(faces, key=lambda face: face[2] * face[3])
        x, y, w, h = largest_face
        
        # ë” ë„“ì€ ë§ˆì§„ìœ¼ë¡œ ì–¼êµ´ ì˜ì—­ í™•ì¥
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
        margin_x = int(w * 0.2)  # ê°€ë¡œ 20% ë§ˆì§„
        margin_y = int(h * 0.3)  # ì„¸ë¡œ 30% ë§ˆì§„ (ë¨¸ë¦¬ í¬í•¨)
        
        x1 = max(0, x - margin_x)
        y1 = max(0, y - margin_y)
        x2 = min(frame.shape[1], x + w + margin_x)
        y2 = min(frame.shape[0], y + h + margin_y)
        
        face_img = frame[y1:y2, x1:x2]
        
        # ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ 
<<<<<<< HEAD
        if face_img.shape[0] > 50 and face_img.shape[1] > 50:
            # í¬ê¸° ì •ê·œí™” (224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)
            face_img = cv2.resize(face_img, (224, 224))
            
            # ëŒ€ë¹„ ë° ë°ê¸° ê°œì„ 
            face_img = cv2.convertScaleAbs(face_img, alpha=1.1, beta=5)
=======
        if face_img.shape[0] > 0 and face_img.shape[1] > 0:
            # í¬ê¸° ì •ê·œí™” (224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)
            face_img = cv2.resize(face_img, (224, 224))
            
            # ëŒ€ë¹„ ê°œì„ 
            face_img = cv2.convertScaleAbs(face_img, alpha=1.2, beta=10)
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
            
        return face_img
    
    def analyze_emotion(self, face_img, save_to_history=True):
        """ê°ì • ë¶„ì„ (íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜µì…˜ ì¶”ê°€)"""
        if face_img is None or self.emotion_pipeline is None:
            return None, None
            
        try:
            # OpenCV BGRì„ RGBë¡œ ë³€í™˜
            rgb_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(rgb_img)
            
<<<<<<< HEAD
            # ê°ì • ë¶„ì„ ì‹¤í–‰
=======
            # ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ëª¨ë‘ í™•ì¸
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
            results = self.emotion_pipeline(pil_img)
            
            if results and len(results) > 0:
                # ëª¨ë“  ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print("ğŸ­ ì „ì²´ ê°ì • ë¶„ì„ ê²°ê³¼:")
<<<<<<< HEAD
                for i, result in enumerate(results[:3]):  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
=======
                for i, result in enumerate(results[:5]):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
                    raw_label = result["label"]
                    mapped_label = EMOTION_MAPPING.get(raw_label.lower(), raw_label.lower())
                    confidence = result["score"]
                    print(f"  {i+1}. {raw_label} -> {mapped_label}: {confidence*100:.2f}%")
                
                # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ê²°ê³¼ ì„ íƒ
                best_result = results[0]
                raw_emotion = best_result["label"]
                emotion = EMOTION_MAPPING.get(raw_emotion.lower(), raw_emotion.lower())
                score = float(best_result["score"])
                
                # ê¸°ì¡´ í†µê³„ ì—…ë°ì´íŠ¸
                if emotion in self.emotion_stats:
                    self.emotion_stats[emotion] += 1
                else:
                    self.emotion_stats[emotion] = 1
                
                # íˆìŠ¤í† ë¦¬ì— ì €ì¥
                if save_to_history:
                    self.emotion_history.add_record(emotion, score, raw_emotion)
                
                print(f"ğŸ¯ ì„ íƒëœ ê°ì •: {raw_emotion} -> {emotion} ({score*100:.1f}%)")
                print(f"ğŸ“Š ì‹¤ì‹œê°„ í†µê³„: {self.emotion_stats}")
                
                return emotion, score
                
        except Exception as e:
            print(f"âŒ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            
        return None, None
    
    def draw_face_info(self, frame, faces, emotion=None, score=None):
<<<<<<< HEAD
        """ì–¼êµ´ ë°•ìŠ¤ì™€ ê°ì • ì •ë³´ ê·¸ë¦¬ê¸° - ë‹¨ì¼ ì–¼êµ´ìš©"""
        if len(faces) == 0:
            return frame
        
        # ë‹¨ì¼ ì–¼êµ´ ì²˜ë¦¬
        face = faces[0]  # ì´ë¯¸ íŠ¸ë˜ì»¤ë¥¼ í†µí•´ ì„ íƒëœ ì–¼êµ´
        x, y, w, h = face
        
        color = EMOTION_COLORS.get(emotion, (0, 255, 0))
        
        # ì–¼êµ´ ë°•ìŠ¤ (ë” ë‘ê»ê²Œ)
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)
        
        # ê°ì • ë¼ë²¨
        if emotion and score is not None:
            # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ ì¡°ì •
            confidence_color = color if score > 0.6 else (128, 128, 128)
            
            label_text = f"{emotion.upper()} ({score*100:.1f}%)"
            label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
            
            # ë¼ë²¨ ë°°ê²½
            cv2.rectangle(frame, (x, y - 35), (x + label_size[0] + 15, y), confidence_color, -1)
            
            # ë¼ë²¨ í…ìŠ¤íŠ¸
            cv2.putText(frame, label_text, (x + 7, y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # íŠ¸ë˜í‚¹ ìƒíƒœ í‘œì‹œ
            status_text = f"Tracking: {self.face_tracker.missing_frames}/{self.face_tracker.max_missing_frames}"
            cv2.putText(frame, status_text, (x, y + h + 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
=======
        """ì–¼êµ´ ë°•ìŠ¤ì™€ ê°ì • ì •ë³´ ê·¸ë¦¬ê¸°"""
        for (x, y, w, h) in faces:
            color = EMOTION_COLORS.get(emotion, (0, 255, 0))
            
            # ì–¼êµ´ ë°•ìŠ¤ (ë” ë‘ê»ê²Œ)
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)
            
            # ê°ì • ë¼ë²¨
            if emotion and score is not None:
                # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ ì¡°ì •
                confidence_color = color if score > 0.5 else (128, 128, 128)
                
                label_text = f"{emotion.upper()} ({score*100:.1f}%)"
                label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
                
                # ë¼ë²¨ ë°°ê²½
                cv2.rectangle(frame, (x, y - 35), (x + label_size[0] + 15, y), confidence_color, -1)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                cv2.putText(frame, label_text, (x + 7, y - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
        return frame
    
    def draw_history_info(self, frame, show_recent=True):
        """í™”ë©´ì— íˆìŠ¤í† ë¦¬ ì •ë³´ í‘œì‹œ"""
        if show_recent:
            recent_records = self.emotion_history.get_recent_records(5)  # ìµœê·¼ 5ë¶„
            if recent_records:
                # ìµœê·¼ ê°ì • ë³€í™” í‘œì‹œ
                y_start = 120
                cv2.putText(frame, "Recent Emotions:", (WIDTH - 200, y_start),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                
                for i, record in enumerate(recent_records[-3:]):  # ìµœê·¼ 3ê°œë§Œ
                    emotion = record["emotion"]
                    score = record["score"]
                    time_str = record["datetime"][-8:-3]  # HH:MMë§Œ
                    
                    color = EMOTION_COLORS.get(emotion, (255, 255, 255))
                    text = f"{time_str} {emotion[:4].upper()} {score*100:.0f}%"
                    
                    cv2.putText(frame, text, (WIDTH - 200, y_start + 20 + i*15),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        return frame
    
    def save_result_locally(self, emotion, score):
        """ê²°ê³¼ë¥¼ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥ (íˆìŠ¤í† ë¦¬ í¬í•¨)"""
        result = {
            "emotion": emotion,
            "score": score,
            "timestamp": time.time(),
            "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "emotion_stats": self.emotion_stats.copy(),
            "recent_history": self.emotion_history.get_recent_records(10),  # ìµœê·¼ 10ë¶„
            "total_records": len(self.emotion_history.history)
        }
        
        with open("latest_emotion_result.json", "w", encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    
    def run(self):
        """ë©”ì¸ ë£¨í”„ ì‹¤í–‰"""
        cap = cv2.VideoCapture(CAM_INDEX)
        if not cap.isOpened():
            raise RuntimeError("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)
        
        # ì¹´ë©”ë¼ ì„¤ì • ê°œì„ 
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_BRIGHTNESS, 0.5)
        cap.set(cv2.CAP_PROP_CONTRAST, 0.5)
        
<<<<<<< HEAD
        window_name = "Real-time Emotion Analysis (Single Face Tracking)"
=======
        window_name = "Real-time Emotion Analysis"
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
        cv2.namedWindow(window_name)
        
        last_emotion, last_score = None, None
        frame_count = 0
        show_history = True
        
<<<<<<< HEAD
        print("ğŸ¥ ì›¹ìº  ì‹œì‘! (ë‹¨ì¼ ì–¼êµ´ ì¶”ì  ëª¨ë“œ)")
        print(f"ğŸŒ ê²°ê³¼ í˜ì´ì§€: {DEPLOYED_APP_URL}")
        print("ğŸ’¡ íŒ: ì¹´ë©”ë¼ ì•ì—ì„œ ë‹¤ì–‘í•œ í‘œì •ì„ ì§€ì–´ë³´ì„¸ìš”!")
        print("âŒ¨ï¸  ì¡°ì‘ë²•:")
        print("  ESC: ì¢…ë£Œ")
        print("  R: ì–¼êµ´ íŠ¸ë˜í‚¹ ë¦¬ì…‹")
=======
        print("ì›¹ìº  ì‹œì‘!")
        print(f"ê²°ê³¼ í˜ì´ì§€: {DEPLOYED_APP_URL}")
        print("íŒ: ë‹¤ì–‘í•œ í‘œì •ì„ ì§€ì–´ë³´ì„¸ìš” (ì›ƒìŒ, í™”ë‚¨, ìŠ¬í”” ë“±)")
        print("ì¡°ì‘ë²•:")
        print("  ESC: ì¢…ë£Œ")
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                    
                frame_count += 1
<<<<<<< HEAD
                faces = self.detect_faces(frame)  # ì´ë¯¸ ë‹¨ì¼ ì–¼êµ´ë§Œ ë°˜í™˜
                
                # ê°ì • ë¶„ì„ (15í”„ë ˆì„ë§ˆë‹¤, íˆìŠ¤í† ë¦¬ëŠ” ëœ ìì£¼)
                if frame_count % 15 == 0 and len(faces) > 0:
                    face_img = self.crop_and_preprocess_face(frame, faces)
                    # íˆìŠ¤í† ë¦¬ ì €ì¥ì€ 45í”„ë ˆì„ë§ˆë‹¤ë§Œ (ë„ˆë¬´ ë§ì€ ë°ì´í„° ë°©ì§€)
                    save_history = frame_count % 45 == 0
=======
                faces = self.detect_faces(frame)
                
                # ë” ìì£¼ ê°ì • ë¶„ì„ (10í”„ë ˆì„ë§ˆë‹¤, íˆìŠ¤í† ë¦¬ëŠ” ëœ ìì£¼)
                if frame_count % 10 == 0 and len(faces) > 0:
                    face_img = self.crop_and_preprocess_face(frame, faces)
                    # íˆìŠ¤í† ë¦¬ ì €ì¥ì€ 30í”„ë ˆì„ë§ˆë‹¤ë§Œ (ë„ˆë¬´ ë§ì€ ë°ì´í„° ë°©ì§€)
                    save_history = frame_count % 30 == 0
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
                    emotion, score = self.analyze_emotion(face_img, save_to_history=save_history)
                    if emotion:
                        last_emotion = emotion
                        last_score = score
                        # ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥
                        self.save_result_locally(emotion, score)
                
                # ì–¼êµ´ ì •ë³´ í‘œì‹œ
                frame = self.draw_face_info(frame, faces, last_emotion, last_score)
                
                # íˆìŠ¤í† ë¦¬ ì •ë³´ í‘œì‹œ
                if show_history:
                    frame = self.draw_history_info(frame)
                
                # ìƒíƒœ ì •ë³´ í‘œì‹œ
<<<<<<< HEAD
                face_status = f"Face: {'âœ“' if len(faces) > 0 else 'âœ—'}"
                if len(faces) > 0:
                    missing = self.face_tracker.missing_frames
                    face_status += f" (Missing: {missing})"
                
                emotion_status = ""
                if last_emotion:
                    emotion_status = f"Emotion: {last_emotion.upper()} ({(last_score or 0)*100:.1f}%)"
                else:
                    emotion_status = "Detecting emotions..."
                
                status = f"{face_status} | {emotion_status} | Records: {len(self.emotion_history.history)}"
                
                cv2.putText(frame, status, (10, HEIGHT - 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # ì‚¬ìš©ë²• ì•ˆë‚´
                help_text = "Single Face Tracking | ESC: quit | R: reset tracking"
                cv2.putText(frame, help_text, (10, HEIGHT - 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
                
                cv2.imshow(window_name, frame)
                
                # í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC í‚¤
                    print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                    break
                elif key == ord('r') or key == ord('R'):  # R í‚¤ë¡œ íŠ¸ë˜í‚¹ ë¦¬ì…‹
                    print("ğŸ”„ ì–¼êµ´ íŠ¸ë˜í‚¹ ë¦¬ì…‹")
                    self.face_tracker.reset()
                    last_emotion, last_score = None, None
=======
                status = f"Faces: {len(faces)} | "
                if last_emotion:
                    status += f"Emotion: {last_emotion.upper()} ({(last_score or 0)*100:.1f}%)"
                else:
                    status += "Detecting emotions..."
                    
                status += f" | Records: {len(self.emotion_history.history)}"
                
                cv2.putText(frame, status, (10, HEIGHT - 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # ì‚¬ìš©ë²• ì•ˆë‚´
                help_text = "Real-time emotion detection | ESC to quit"
                cv2.putText(frame, help_text, (10, HEIGHT - 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
                
                cv2.imshow(window_name, frame)
                
                # ESC í‚¤ë¡œ ì¢…ë£Œ
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC í‚¤
                    break
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
                    
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„ ì¶œë ¥
<<<<<<< HEAD
            print("\nğŸ ì„¸ì…˜ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ê¸°ë¡ëœ ê°ì •: {len(self.emotion_history.history)}ê°œ")
            print(f"ğŸ­ ì‹¤ì‹œê°„ í†µê³„: {self.emotion_stats}")
=======
            print("\nì„¸ì…˜ ì™„ë£Œ!")
            print(f"ì´ ê¸°ë¡ëœ ê°ì •: {len(self.emotion_history.history)}ê°œ")
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760
            self.emotion_history.print_summary(60)  # ìµœê·¼ 1ì‹œê°„


def main():
<<<<<<< HEAD
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ­ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ê¸° (ê°œì„ ëœ ë²„ì „)")
    print("=" * 60)
    print("âœ¨ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("  - ëª¨ë¸ ì¤‘ë³µ ë¡œë”© ë°©ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)")
    print("  - ë‹¨ì¼ ì–¼êµ´ ì•ˆì •ì  ì¶”ì ")
    print("  - í–¥ìƒëœ ì–¼êµ´ ì¸ì‹ ì •í™•ë„")
    print("  - íŠ¸ë˜í‚¹ ìƒíƒœ ì‹œê°í™”")
    print()
    print(f"ğŸŒ ê²°ê³¼ í˜ì´ì§€: {DEPLOYED_APP_URL}")
    print("ğŸ’¡ ë‹¤ì–‘í•œ ê°ì • í‘œí˜„ì„ ì‹œë„í•´ë³´ì„¸ìš”!")
    print("=" * 60)
    
    try:
        analyzer = WebcamEmotionAnalyzer()
        analyzer.run()
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ğŸ”š í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
=======
    print("ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ê¸° ì‹œì‘...")
    print(f"ê²°ê³¼ í˜ì´ì§€: {DEPLOYED_APP_URL}")
    print("ë‹¤ì–‘í•œ ê°ì • í‘œí˜„ì„ ì‹œë„í•´ë³´ì„¸ìš”!")
    
    analyzer = WebcamEmotionAnalyzer()
    analyzer.run()
>>>>>>> f81054bf3e5cea299f28fc141e2bd08a1635d760


if __name__ == "__main__":
    main()